{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":33107,"databundleVersionId":3943986,"sourceType":"competition"},{"sourceId":23404,"sourceType":"datasetVersion","datasetId":17860}],"dockerImageVersionId":30207,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.1 Objectives</b></p>\n</div>\n\nIn this notebook, we'll be covering what a **Gaussian Mixture Model** (GMM) is and **implementing it from scratch**. I will be placing particular emphasis on building **intuition through examples** in 1 dimension and later generalising the equations to $d$ dimensions. \n\nThe notebook is structured as follows:\n\n* Probability/Statistics review\n* Gaussian Mixture **examples**\n* EM algorithm in **1 dimension**\n* EM algorithm in **d dimensions**\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.2 History</b></p>\n</div>\n\nThe idea of using **Gaussian mixtures** was popularised by Duda and Hart in their seminal 1973 text, '[Pattern Classification and Scene Analysis](https://www.semanticscholar.org/paper/Pattern-classification-and-scene-analysis-Duda-Hart/b07ce649d6f6eb636872527104b0209d3edc8188)'. However, it was not until 1977 that the **Expectation-Maximisation (EM)** was presented by Dempster, Laird and Rubin in their paper, '[Maximum Likelihood from Incomplete Data Via the EM Algorithm](https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2517-6161.1977.tb01600.x)'. \n\nAs we will see later, the EM algorithm is the **learning algorithm** behind a Gaussian Mixture Model (GMM), i.e. it solves the inverse problem by finding the **parameters** of the model.\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.3 Motivation</b></p>\n</div>\n \nYou might be familiar with **k-Means**, which is a clustering algorithm that identifies k centroids and **allocates** data points based on the **nearest centroid**. Whilst k-Means is a **simple and fast** algorithm, it's main drawback is that it produces **spherical clusters**, which can model the data poorly. \n\nThis is where **GMMs** come in. We will see that they produce clusters with **Gaussian distributions**, which are much more **flexible, and reflective** of real-life data.\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>1.4 Libraries</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import cm\nfrom itertools import combinations\nimport math\nimport statistics\nfrom scipy import stats\nfrom scipy.stats import pearsonr\nfrom scipy.stats import shapiro\nfrom scipy.stats import chi2\nfrom scipy.stats import poisson\nfrom scipy.stats import multivariate_normal\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\nimport plotly.express as px\nfrom termcolor import colored\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n\n# UMAP\nimport umap\nimport umap.plot","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-22T20:17:32.870988Z","iopub.execute_input":"2023-07-22T20:17:32.872557Z","iopub.status.idle":"2023-07-22T20:18:09.895383Z","shell.execute_reply.started":"2023-07-22T20:17:32.872418Z","shell.execute_reply":"2023-07-22T20:18:09.893445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Prerequisites\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>2.1 Normal distribution in 1 dimension</b></p>\n</div>\n\nThe **normal distribution** (or Gaussian distribution) is a continuous probability distribution characterised by its **bell shape**. It has the **probability density function** given by\n\n$$\nf_{\\mathcal{N}(\\mu, \\sigma^2)} (x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(x-\\mu)^2}{2 \\sigma^2} \\right)\n$$\n\nwhere\n\n* $x$ is a real number\n* $\\mu$ is the mean\n* $\\sigma$ is the standard deviation\n* $\\pi=3.141592653 \\ldots$.\n\n<center>\n<img src=\"https://cdn.kastatic.org/ka-perseus-graphie/191a8f604b04f7b6e4a80d04db881c12798856f7.svg\" width=\"400\">\n</center>\n\nThe normal distribution appears all the time in real life due to the **Central Limit Theorem**. This states that under certain conditions, the averages of many samples of a random variable **converge** to a normal distribution.\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>2.2 Normal distribution in d dimensions</b></p>\n</div>\n\nThe **multivariate normal distribution** is the generalisation of the normal disitribution to $d$ dimensions. This time, the **probability density function** is given by\n\n$$\nf_{\\mathcal{N}({\\boldsymbol \\mu}, {\\boldsymbol \\Sigma})} (\\textbf{x}) = \\frac{1}{\\sqrt{(2 \\pi)^d \\det({\\boldsymbol \\Sigma})}} \\exp \\left(-\\frac{(\\textbf{x}-{\\boldsymbol \\mu})^T {\\boldsymbol \\Sigma}^{-1} (\\textbf{x}-{\\boldsymbol \\mu})}{2} \\right)\n$$\n\nwhere \n\n* $\\textbf{x} = (x_1, \\ldots, x_d)$ is a vector of length $d$\n* ${\\boldsymbol \\mu} = (\\mu_1, \\ldots, \\mu_d)$ is the mean vector\n* ${\\boldsymbol \\Sigma}$ is the covariance matrix of size $d \\times d$\n* $\\det({\\boldsymbol \\Sigma})$ is the determinant of the covariance matrix.\n\n<center>\n<img src=\"https://se.mathworks.com/help/examples/stats/win64/ComputeTheMultivariateNormalPdfExample_01.png\" width=\"400\">\n</center>\n\nThe main difference to the univariate case, is that each feature/dimension can be **correlerated** with the other ones. These correlations are modelled by the **covariance matrix**, which allows for the density function to be **rotated** and **stretched** to model the data as well as possible.\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>2.3 Bayes' theorem</b></p>\n</div>\n\n**Bayes' theorem** is incredibly important. It allows us to calculate the **probability of an event** based on **prior knowledge** related to the event. In its simplest form, the theorem states that\n\n$$\n\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(B | A) \\mathbb{P}(A)}{\\mathbb{P}(B)}\n$$\n\nwhenever the denominator is **well-defined** (i.e. not 0). In **Bayesian statistics** these terms are given technical names:\n\n* $\\mathbb{P}(A)$ is called the **prior**\n* $\\mathbb{P}(B | A)$ is called the **likelihood**\n* $\\mathbb{P}(A | B)$ is called the **posterior**\n* $\\mathbb{P}(B)$ is called the **evidence**.\n\n<hr>\n\nAn **equivalent formula**, which we will use later, is given by \n\n$$\n\\mathbb{P}(A | B) = \\frac{\\mathbb{P}(B | A) \\mathbb{P}(A)}{\\mathbb{P}(B | A) \\mathbb{P}(A)+\\mathbb{P}(B | A^{c}) \\mathbb{P}(A^{c})}\n$$\n\nwhere the event $A^{c}$ is the **complement** of $A$. This is derived by using the the **Law of Total Probability**. This formula is useful when you need to **compare two posterior distributions** and choose the most probable one. \n\n<hr>\n\nThe more **general version** of this formula, which we will also use, is given by\n\n$$\n\\mathbb{P}(A_j | B) = \\frac{\\mathbb{P}(B | A_j) \\mathbb{P}(A_j)}{\\sum_{t} \\mathbb{P}(B | A_t) \\mathbb{P}(A_t)}.\n$$\n\nThis is used when you need to **compare several posterior distributions** and choose the most probable one. ","metadata":{}},{"cell_type":"markdown","source":"# 3. Gaussian Mixtures\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.1 Definition</b></p>\n</div>\n\nA **Gaussian Mixture** is simply a combination (or mixture) of Gaussian distributions. Note that a Guassian distribution is the same as a **Normal** distribution and I will use these terms interchangeably. It is a **common** distribution that appears in **clustering** problems where the data is sampled randomly from several groups that each follow a normal distribution. \n\n<hr>\n\nIn **1 dimension**, the model is a weighted sum of **univariate** normal distributions:\n\n$$\nf_{GMM} (x) = \\sum_{j=1}^{k} \\pi_j f_{\\mathcal{N}(\\mu_j, \\sigma_{j}^2)} (x)\n$$\n\nwhere \n\n* k = number of clusters\n* x is a real number\n* $f_{\\mathcal{N}(\\mu, \\sigma^2)}$ is the density of a normal distribution with mean $\\mu$ and variance $\\sigma^2$\n* $\\pi = (\\pi_1, ..., \\pi_k)$ are the weights subject to\n\n$$\n0 \\leq \\pi_j \\leq 1, \\quad \\sum_{j=1}^{k} \\pi_j = 1.\n$$\n\n<hr>\n\nIn **d dimensions**, the model is a weighted sum of **multivariate** normal distributions:\n\n$$\nf_{GMM} (\\textbf{x}) = \\sum_{j=1}^{k} \\pi_j f_{\\mathcal{N}({\\boldsymbol \\mu}_j, {\\boldsymbol \\Sigma}_{j})} (\\textbf{x})\n$$\n\nwhere\n\n* $\\textbf{x} = (x_1, \\ldots, x_d)$ is now a vector of length $d$\n* $f_{\\mathcal{N}({\\boldsymbol \\mu}_j, {\\boldsymbol \\Sigma}_{j})}$ is the density of a **multivariate** normal distribution with mean vector ${\\boldsymbol \\mu}$ and covariance matrix ${\\boldsymbol \\Sigma}$\n* $\\pi = (\\pi_1, ..., \\pi_k)$ are the weights subject to the same conditions as before.\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.2 Example in 1D</b></p>\n</div>\n\nSuppose that every month for a year, you **survey random people** from the population and ask them whether they **approve** of the prime minister/president or not. I'm from the UK so we will use Boris Johnson as an example. As you might expect, people who **support** his party (the Conservatives) are **more likely** to approve of him, whilst people who support the main **opposition** party (Labour) are **less likely** to approve of him. The distribution produced is a Gaussian Mixture.","metadata":{}},{"cell_type":"code","source":"# Monthly approval ratings for Boris Johnson (dates: Aug 2021- July 2022, source: YouGov)\ndf_approval = pd.DataFrame({'Conservative': [67,70,66,57,47,46,50,60,59,55,51,50], 'Labour': [11,7,7,6,6,4,6,4,6,6,5,4]})\n\n# Plot data\nplt.figure(figsize=(18,4))\nplt.subplot(1,2,1)\nsns.histplot(df_approval, binwidth=1, kde=True, kde_kws={'bw_adjust': 2})\nplt.title('Approval ratings by party')\n\nplt.subplot(1,2,2)\nsns.histplot(pd.concat([df_approval['Conservative'],df_approval['Labour']]), binwidth=1, kde=True, kde_kws={'bw_adjust': 0.5})\nplt.title('Approval ratings (mixed)')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-22T20:18:09.898825Z","iopub.execute_input":"2023-07-22T20:18:09.900608Z","iopub.status.idle":"2023-07-22T20:18:10.995669Z","shell.execute_reply.started":"2023-07-22T20:18:09.90054Z","shell.execute_reply":"2023-07-22T20:18:10.994114Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If we included data of **additional** parties (like Liberal Democrats, Greens, etc) then there could be more '**bumps**' in this picture. \n\nThe **inverse problem**, which we will discuss soon, is more interesting. Given the plot on the right, how can we **infer** which data points correspond to Labour supporters and which ones correspond to Conservative supporters. That is, how do we **identify the clusters**, if all we know is that there should be 2 of them and they should be normally distributed. \n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.3 Example in 2D</b></p>\n</div>\n\nFor this example, we will select 2 features from the famous **Iris dataset**. This contains observations of flowers, like petal lengths, for **3 different species**, so k=3.","metadata":{}},{"cell_type":"code","source":"# Load in data\ndf_iris = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\n\n# Select 2 features\ndf_iris = df_iris[['sepal_length','petal_length','species']]\n\n# Scatterplot\nfig = plt.figure(figsize=(20,6))\nfig.add_subplot(121)\nax1 = sns.scatterplot(data=df_iris, x='sepal_length', y='petal_length', hue='species')\nax1.set_title('2 dimensional data with 3 clusters')\n\n'''3D plot for 2D gaussian mixture estimation'''\n# Define grid\nx = np.linspace(4,9, num=100)\ny = np.linspace(0,8, num=100)\nx, y = np.meshgrid(x, y)\nz = 0\n\n# Estimate gaussian parameters\nfor flower in ['Iris-setosa','Iris-versicolor','Iris-virginica']:\n    mu = df_iris[df_iris['species']==flower].iloc[:,:-1].mean()\n    sigma = df_iris[df_iris['species']==flower].iloc[:,:-1].cov()\n    z += multivariate_normal.pdf(np.dstack((x, y)), mu, sigma)\n\n# Plot 3D plot with gaussian mixture estimate\nax2 = fig.add_subplot(122, projection='3d')\nax2.plot_surface(x,y,z, cmap=cm.jet)\nax2.set_title('2D Gaussian mixture plotted in 3D')\nax2.set_xlabel('sepal_length')\nax2.set_ylabel('petal_length')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-22T20:18:10.998933Z","iopub.execute_input":"2023-07-22T20:18:10.999889Z","iopub.status.idle":"2023-07-22T20:18:12.346262Z","shell.execute_reply.started":"2023-07-22T20:18:10.999825Z","shell.execute_reply":"2023-07-22T20:18:12.345136Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The **inverse problem** this time, is this. Given the **density plot** on the right, and the fact that we're searching for 3 clusters, how do we decide which data points belong **together** (i.e. correspond to the same flower), and which ones don't. \n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.4 Sampling from a mixture</b></p>\n</div>\n\nTo **sample** from a Gaussian mixture we can view this as a **two-step generative process**:\n* Sample a random **class** index $C_i$ from the **categorical distribution** parameterised by $\\pi=(\\pi_1, \\ldots, \\pi_k)$.\n* Then, sample a random **observation** $\\textbf{X}_i$ from the corresponding **normal distribution** associated with class $C_i$.\n\nSymbolically, we can represent this as\n\n<center>\n<img src=\"https://i.postimg.cc/m2Gp7MqZ/CXdistributions.png\" width=\"180\">\n</center>\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.5 The inverse problem</b></p>\n</div>\n\nNow comes the exciting part. If we think that the data $X$ is distributed according to a **Gaussian mixture** with $k$ classes, how do we cluster this data so that we end up with $k$ normal distributions? That is, how do we estimate the **parameters** (means and covariances) of each of the $k$ normal distributions.\n\n<hr>\n\n**Scenario 1:**\nIf we **knew the class** that each data point belonged to, then we could **work out the parameters** of each normal distribution. This can be done by using formulas like this: (we'll formalise this later)\n\n<center>\n<img src=\"https://i.postimg.cc/Sx9K3Bdf/musigmaformula.png\" width=\"300\">\n</center>\n\non a class-by-class basis ($\\, j=1, \\ldots, k$).\n\n<hr>\n\n**Scenario 2:**\nIf we **knew the parameters**, then we could **work out which class** each point should belong to. This can be done by using formulas like this: (we'll formalise this later)\n\n$$\n\\mathbb{P}(C_j | x_i) = \\frac{\\mathbb{P}(x_i|C_j) \\mathbb{P}(C_j)}{\\sum_{t=1}^{k} \\mathbb{P}(x_i|C_t) \\mathbb{P}(C_t)}\n$$\n\nfor each data point ($\\, i=1, \\ldots, n$).\n\n<hr>\n\n**The reality:**\n\nUnfortunately, we are not quite in either of these scenarios but **if one was true**, then we would also be able to **perform the other one**. The solution to this **chicken and egg problem** is called the **Expectation-Maximisation (EM) algorithm**. \n\n<br>\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>3.5 EM algorithm at a glance</b></p>\n</div>\n\nThe EM algorithm works in a **similar way to k-Means**. We start by selecting a **random** set of parameters for the $k$ gaussian distributions. This allows us to work out which class every point should belong to (scenario 2, called the **E-step**). The difference to k-Means however, is that EM performs a **soft assignment** (as opposed to a hard assignment). In k-Means, we label each data points as coming from one of the k classes, whereas in EM, we find the **probability** that each data point belongs to each of the $k$ classes. These probabilities are then used to **update** the Gaussian parameters (scenario 1, called the **M-step**). Finally, we **iterate to convergence**, always alternating between the E-step and the M-step.","metadata":{}},{"cell_type":"markdown","source":"# 4. Expectation-Maximisation algorithm\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>4.1 Example in 1D</b></p>\n</div>\n\nWe'll start by building the **intuition** on how EM works by working through an example in 1 dimension. We will then formalise the general algorithm in d dimensions afterwards. \n\n\nLet's start with some **data** that we think has been generated by a **Gaussian mixture** with **k=2** classes in **1 dimension**.\n\n<center>\n<img src=\"https://i.postimg.cc/76W5fXTY/62420.jpg\" width=\"700\">\n</center>\n\n## Initialisation\n\nThe **first task** for the algorithm is to **randomly choose parameters** $\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2$ for the two normal distributions. We also need to choose some **weights** $\\pi_1, \\pi_2$, which tells us the **relative heights** of the curves. A good choice for the initial weights is $(\\pi_1, \\pi_2)=(0.5,0.5)$, i.e. the uniform distribution.\n\nThe resulting normal distributions might look something like this.\n \n<center>\n<img src=\"https://i.postimg.cc/9XgkQ988/20957.jpg\" width=\"700\">\n</center>\n\n## E-step\n\nThe **next step** (called the E-step) is to calculate how likely does each data point belong to each **class**. For example, consider the **rightmost point** on the diagram. Even though it is very **far from both** the green and orange distributions (and so is **unlikely** to have been generated from either of them), it is much **closer** to the green one. This means it is **more likely** to have been generated from the green distribution than the orange one, even if these probabilites are small. \n\n<hr>\n\n**Bayes' formula** uses this idea of likelihood to find the probabilities we want:\n\n$$\na_i = \\mathbb{P}(C_1 | x_i) = \\frac{\\mathbb{P}(x_i|C_1) \\mathbb{P}(C_1)}{\\mathbb{P}(x_i|C_1) \\mathbb{P}(C_1) + \\mathbb{P}(x_i|C_2) \\mathbb{P}(C_2)} = \\frac{\\mathbb{P}(x_i|C_1) \\pi_1}{\\mathbb{P}(x_i|C_1) \\pi_1 + \\mathbb{P}(x_i|C_2) \\pi_2}\n$$\n\nwhere the **likelihood** is given by\n\n$$\n\\mathbb{P}(x_i|C_1) = \\frac{1}{\\sqrt{2 \\pi \\sigma_1^2}} \\exp \\left(-\\frac{(x_i-\\mu_1)^2}{2 \\sigma_1^2} \\right)\n$$\n\n<hr>\n\nNote that since there are only $k=2$ classes, we have $b_i = \\mathbb{P}(C_2 | x_i) = 1 - \\mathbb{P}(C_1 | x_i) = 1-a_i$. \n\nE.g. if $a_i = 0.99$ then $b_i = 0.01$. \n\n<hr>\n\nSo now we can think of the data points as being a **bit of orange** and a **bit of green** together. The exact proportion depends on the probabilities.\n\n<center>\n<img src=\"https://i.postimg.cc/X7LHZCYB/75482.jpg\" width=\"700\">\n</center>\n\n## M-step\n\nNow we can **update the parameters** (called the M-step). Unlike in k-Means, which assigns classes using the **maximum** of these probabilites, GMMs use the **probabilities** to estimate the means and variances as weighted averages. \n\nFor the **orange** distribution:\n\n$$\n\\mu_1 = \\frac{\\sum_{i=1}^{n} a_i x_i}{\\sum_{i=1}^{n} a_i}, \\quad \\sigma_1^2 = \\frac{\\sum_{i=1}^{n} a_i (x_i - \\mu_1)^2}{\\sum_{i=1}^{n} a_i}\n$$\n\nAnd for the **green** distribution:\n$$\n\\mu_2 = \\frac{\\sum_{i=1}^{n} b_i x_i}{\\sum_{i=1}^{n} b_i}, \\quad \\sigma_2^2 = \\frac{\\sum_{i=1}^{n} b_i (x_i - \\mu_1)^2}{\\sum_{i=1}^{n} b_i}\n$$\n\nThese are just **weighted averages** of the usual formulas for mean and variance. The idea is that points **closest** to the distribution **contribute the most** to updating the correspoding parameters. \n\n<hr>\n\nAnd we can't forget about updating the weights as well. This is done by **averaging** over the posterior probabilities:\n\n$$\n\\pi_1 = \\frac{\\sum_{i=1}^{n} a_i}{n}, \\quad \\pi_2 = \\frac{\\sum_{i=1}^{n} b_i}{n} \n$$\n\nYou can think of this as calculating the **proportion** of the data that each cluster is describing. \n\n<hr>\n\nAnd all that is left to do is to **iterate** until convergence! \n\n<center>\n<img src=\"https://i.postimg.cc/HWXbnct0/67259.jpg\" width=\"700\">\n</center>\n\n<br>\n\n<center>\n<img src=\"https://i.postimg.cc/7Zs2dgx4/59430.jpg\" width=\"700\">\n</center>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>4.2 General case in d dimensions</b></p>\n</div>\n\nNow that you hopefully have the intuition behind GMMs, we are going to **generalise** the EM algorithm to **d dimensions** and **k clusters**. You will see that the equations are very similar, albeit more complicated, but their purpose is the same as before. \n\n<center>\n<img src=\"https://c.tenor.com/i1rNMdaKd7MAAAAC/gaussian-mixture-models-em-method-math.gif\" width=\"400\">\n</center>\n\n## Initialisation\n\nThere are many options to initialise the means, covariance matrices and weights of the model. A **popular approach** (but not necessarily the best) is this. For each cluster k, choose the **mean** to be a **random data point** and the **covariance matrix** to be the **covariance of the dataset** $X$. The **weights** $\\pi = (\\pi_1, \\ldots, \\pi_k)$ are usually chosen to be **uniformly distributed**. \n\nAnother approach worth mentioning is to use the output of the **k-Means algorithm** to initialise the parameters. The choice of initialisation can have **big impacts** on the resulting **clusters**, so it is worth experimenting with different techniques. \n\n## E-step\n\n\nThe posterior probabilities this time are called **responsibilities**, $r_{ij}$, of the $j$-th mixture component for data point $x_i$. These generalise the quantities $a_i$ and $b_i$ we saw earlier.\n\n$$\nr_{ij} = \\mathbb{P}(C_j | \\textbf{x}_i) = \\frac{\\mathbb{P}(\\textbf{x}_i|C_j) \\mathbb{P}(C_j)}{\\sum_{t=1}^{k} \\mathbb{P}(\\textbf{x}_i|C_t) \\mathbb{P}(C_t)} = \\frac{\\mathbb{P}(\\textbf{x}_i|C_j) \\pi_j}{\\sum_{t=1}^{k} \\mathbb{P}(\\textbf{x}_i|C_t) \\pi_t}\n$$\n\n## M-step\n\nUnlike in **gradient descent**, which performs **small improvements** towards the direction of the optimum solution, the **EM algorithm** jumps straight the **best solution** available at each step. It's for this reason the algorithm can **converge** in only a few number of steps but it also means that the **starting conditions** have a huge impact on the result. \n\nWe estimate the **mean** and **covariances** of each class as follows:\n\n$$\n{\\boldsymbol \\mu}_j = \\frac{\\sum_{i=1}^n r_{ij} \\textbf{x}_{i}}{n_j}, \\qquad {\\boldsymbol \\Sigma}_j = \\frac{1}{n_j} \\sum_{i=1}^{n} r_{ij} (\\textbf{x}_i - {\\boldsymbol \\mu}_j) (\\textbf{x}_i - {\\boldsymbol \\mu}_j)^{T}\n$$\n\nwhere $n_j = \\sum_{i=1}^{n} r_{ij}$ is defined as the **total responsibility** of the j-th mixture component.\n\nThese generalise our earlier formulas. Remember, they are just **weighted averges** of the usual mean and covariance formulas. Points closest to each cluster have the biggest **impact** on updating the corresponding Gaussian parameters.\n<hr>\n\nFinally, we update the **mixture weights** to be\n\n$$\n\\pi_j = \\frac{n_j}{n}.\n$$\n\nThink of this as the **proportion of the total responsibility** associated with the $j$-th mixture. \n\n<hr>\n\nAnd **that's it**! Surprisingly, the EM-algorithm needs just a **handful of equations** to be implemented. We'll do this next.","metadata":{}},{"cell_type":"markdown","source":"# 5. Implementation\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>5.1 GMM</b></p>\n</div> \n\nI adapted the code from this blog: [ML From Scratch, Part 5: Gaussian Mixture Models](http://www.oranlooney.com/post/ml-from-scratch-part-5-gmm/).","metadata":{}},{"cell_type":"code","source":"class GMM:\n    def __init__(self, k, max_iter=100, random_state = 0):\n        self.k = k\n        self.max_iter = max_iter\n        self.random_state = random_state\n\n    def initialise(self, X):\n        self.shape = X.shape\n        self.n, self.d = self.shape\n        \n        self.pi = np.full(shape=self.k, fill_value=1/self.k)\n        self.responsibilities = np.full(shape=self.shape, fill_value=1/self.k)\n        \n        np.random.seed(self.random_state)\n        random_row = np.random.randint(low=0, high=self.n, size=self.k)\n        self.mu = [X[row_index,:] for row_index in random_row]\n        self.sigma = [np.cov(X.T) for _ in range(self.k)]\n\n    def E_step(self, X):\n        # E-Step: update the responsibilities by holding mu and sigma constant\n        self.responsibilities = self.predict_proba(X)\n    \n    def M_step(self, X):\n        # M-Step: update pi, mu and sigma by holding responsibilities constant\n        self.pi = self.responsibilities.mean(axis=0)\n        for j in range(self.k):\n            r_column = self.responsibilities[:,j]\n            total_responsibility = r_column.sum()\n            self.mu[j] = (X * r_column[:, np.newaxis]).sum(axis=0)/total_responsibility\n            self.sigma[j] = np.cov(X.T, aweights=(r_column/total_responsibility).flatten(), bias=True)\n\n    def fit(self, X):\n        self.initialise(X)\n        \n        for iteration in range(self.max_iter):\n            self.E_step(X)\n            self.M_step(X)\n    \n    def predict_proba(self, X):\n        likelihood = np.zeros((self.n, self.k))\n        for j in range(self.k):\n            distribution = multivariate_normal(mean=self.mu[j], cov=self.sigma[j])\n            likelihood[:,j] = distribution.pdf(X)\n        \n        numerator = likelihood * self.pi\n        denominator = numerator.sum(axis=1)[:, np.newaxis]\n        responsibilities = numerator / denominator\n        return responsibilities\n    \n    def predict(self, X):\n        responsibilities = self.predict_proba(X)\n        return np.argmax(responsibilities, axis=1)\n    \n    def fit_predict(self, X):\n        self.fit(X)\n        predictions = self.predict(X)\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2023-07-22T20:18:12.348961Z","iopub.execute_input":"2023-07-22T20:18:12.349906Z","iopub.status.idle":"2023-07-22T20:18:12.372472Z","shell.execute_reply.started":"2023-07-22T20:18:12.349864Z","shell.execute_reply":"2023-07-22T20:18:12.370985Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>5.2 Application</b></p>\n</div> \n\nLet's take our new model for a spin. We will apply it to this months TPS competition which is on **unsupervised clustering**. You can find my detailed analysis on the dataset in my notebook here: [TPS July 22 - Unsupervised clustering](https://www.kaggle.com/code/samuelcortinhas/tps-july-22-unsupervised-clustering).\n\nThe dataset is made up of **29 features** and **98,000** data points and the task is to **cluster the data**.","metadata":{}},{"cell_type":"code","source":"# Load data\ndata=pd.read_csv('../input/tabular-playground-series-jul-2022/data.csv', index_col='id')\n\n# Preprocess data\nscaled_data = pd.DataFrame(PowerTransformer().fit_transform(data))\nscaled_data.columns = data.columns\ndrop_feats = [f'f_0{i}' for i in range(7)]\ndrop_feats = drop_feats + [f'f_{i}' for i in range(14,22)]\nX = scaled_data.drop(drop_feats, axis=1).values","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-22T20:18:12.374763Z","iopub.execute_input":"2023-07-22T20:18:12.375259Z","iopub.status.idle":"2023-07-22T20:18:17.820324Z","shell.execute_reply.started":"2023-07-22T20:18:12.375209Z","shell.execute_reply":"2023-07-22T20:18:17.819101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# Gaussian Mixture Model\ngmm = GMM(k=7, max_iter=100)\npreds = gmm.fit_predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T20:18:17.82236Z","iopub.execute_input":"2023-07-22T20:18:17.822847Z","iopub.status.idle":"2023-07-22T20:19:21.519692Z","shell.execute_reply.started":"2023-07-22T20:18:17.822799Z","shell.execute_reply":"2023-07-22T20:19:21.517995Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>5.3 Visualise predictions</b></p>\n</div> \n\n**Label distribution**","metadata":{}},{"cell_type":"code","source":"# Countplot\nplt.figure(figsize=(10,4))\nsns.countplot(x=preds)\nplt.title('Predicted clusters')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-22T20:19:21.522284Z","iopub.execute_input":"2023-07-22T20:19:21.523461Z","iopub.status.idle":"2023-07-22T20:19:21.832153Z","shell.execute_reply.started":"2023-07-22T20:19:21.523392Z","shell.execute_reply":"2023-07-22T20:19:21.830716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Cluster distributions**\n\nWe know (from my other notebook) that the continuous features are **gaussian mixtures**. We can check to see if the clusters are normally distributed over each feature as they should be.","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/code/ambrosm/tpsjul22-gaussian-mixture-cluster-analysis\nfig, axs = plt.subplots(2, 4, figsize=(20, 7))\naxs = axs.ravel()\nfloat_columns = ['f_22','f_23','f_24','f_25','f_26','f_27','f_28']\ny=preds\nfor ax, f in zip(axs, float_columns):\n    for i in range(7):\n        h, edges = np.histogram(data[f][y == i], bins=np.linspace(-5, 5, 26))\n        ax.plot((edges[:-1] + edges[1:]) / 2, h, label=f\"Cluster {i}\", lw=3)\n    ax.set_title(f)\n#axs[-2].axis('off')\naxs[-1].axis('off')\nplt.suptitle('Histograms of continuous features by cluster', y=1.02, fontsize=28)\nfig.tight_layout(h_pad=1.0, w_pad=0.5)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-22T20:19:21.83356Z","iopub.execute_input":"2023-07-22T20:19:21.83394Z","iopub.status.idle":"2023-07-22T20:19:23.613328Z","shell.execute_reply.started":"2023-07-22T20:19:21.833906Z","shell.execute_reply":"2023-07-22T20:19:23.611936Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**PCA**\n\nPrinciple Component Analysis","metadata":{}},{"cell_type":"code","source":"%%time\n\n# PCA\npca = PCA(n_components=3)\ncomponents = pca.fit_transform(X)\n\n# 3D scatterplot\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=preds, size=0.1*np.ones(len(X)), opacity = 1,\n    title='PCA plot in 3D',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n    width=650, height=500\n)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-22T20:19:23.615098Z","iopub.execute_input":"2023-07-22T20:19:23.616387Z","iopub.status.idle":"2023-07-22T20:19:25.77422Z","shell.execute_reply.started":"2023-07-22T20:19:23.616327Z","shell.execute_reply":"2023-07-22T20:19:25.772422Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>5.4 Submit predictions</b></p>\n</div> \n\nCompetition **public leaderboard** score: (higher is better)\n* My GMM: 0.54838\n* sklearn's GMM: 0.49625","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-jul-2022/sample_submission.csv')\nsub['Predicted'] = preds\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T20:19:25.777097Z","iopub.execute_input":"2023-07-22T20:19:25.778336Z","iopub.status.idle":"2023-07-22T20:19:25.997718Z","shell.execute_reply.started":"2023-07-22T20:19:25.778293Z","shell.execute_reply":"2023-07-22T20:19:25.996337Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems that our random initialisation produced **better results** than sklearn's version, which uses the output of a k-Means run to initialise the parameters.\n\n# 6. Conclusion\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>6.1 Strengths</b></p>\n</div> \n\nGMM is a popular clustering algorithm for the following reasons:\n\n* **Fast** - it's the fastest algorithm for learning mixture models\n* **Adaptable** - can handle a greater variety of shapes than k-Means\n* **Soft classification** - can provide the probability that each point belongs to a particular cluster\n\n<div style=\"color:white;display:fill;\n            background-color:#4577ff;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 4px;color:white;\"><b>6.2 Limitations</b></p>\n</div> \n\nSome of the drawbacks of GMMs include:\n\n* **Number of components** - you have to choose this manualy beforehand\n* **Singularities** - some mixtures can 'die out' if there are no points nearby. This can lead to problems computing the covariance matrices. \n\n#  7. References \n\nThe two **videos** by Victor Lavrenko were great for building my **intuition** and I recommend you watch them too. The **book** contained all the correct **equations** and their derivation. The **articles** were most helpful in writting the **code**.\n\n* [Video: EM algorithm: how it works](https://www.youtube.com/watch?v=REypj2sy_5U&ab_channel=VictorLavrenko) by Victor Lavrenko.\n* [Video: Expectation Maximization: how it works](https://www.youtube.com/watch?v=iQoXFmbXRJA&ab_channel=VictorLavrenko) by Victor Lavrenko.\n* [Article: Gaussian Mixture Models:\nimplemented from scratch](https://towardsdatascience.com/gaussian-mixture-models-implemented-from-scratch-1857e40ea566) by Vasile Păpăluță.\n* [Book: Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf) by Deisenroth, Faidal and Ong, 2020. \n* [Article: ML From Scratch, Part 5: Gaussian Mixture Models](http://www.oranlooney.com/post/ml-from-scratch-part-5-gmm/) by Oran Looney.\n* [Article: Expectation-Maximization for GMMs explained](https://towardsdatascience.com/expectation-maximization-for-gmms-explained-5636161577ca) by Maël Fabien.\n* [Notebook: TPS July 22 - Unsupervised clustering](https://www.kaggle.com/code/samuelcortinhas/tps-july-22-unsupervised-clustering) by [Samuel Cortinhas](https://www.kaggle.com/samuelcortinhas).","metadata":{}}]}